{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4595fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d51f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help classes and functions (same as ANN).\n",
    "##one-hot-encoding\n",
    "class one_hot_encoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,y_train):\n",
    "    #fit y_train, find set{y_train} and assign elements of this set with one-hot-vectors.\n",
    "       label_set = set(y_train)\n",
    "    #assign index to labels:\n",
    "       self.label_list = list(label_set)\n",
    "    \n",
    "    def transform(self,y): #apply one-hot-encoding to y_train or y_test\n",
    "        #the shape of output matrix is (y.shape[0],len(self.label_list))\n",
    "        ohe_matrix = np.zeros((y.shape[0],len(self.label_list)))\n",
    "        for label,vector in zip(y,ohe_matrix):\n",
    "            #find index of label and add 1 to vector[index]\n",
    "            index = self.label_list.index(label)\n",
    "            vector[index]+=1\n",
    "        return ohe_matrix.astype(int)\n",
    "    \n",
    "def ohe(y_train,y_test):\n",
    "    ohencoder = one_hot_encoder()\n",
    "    ohencoder.fit(y_train)\n",
    "    return ohencoder.transform(y_train),ohencoder.transform(y_test)\n",
    "### normalize feature\n",
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X_train): ##fit X_train, calculate mean, std of X_train\n",
    "        self.u = X_train.mean(axis=0)\n",
    "        self.s = X_train.std(axis=0)\n",
    "    def transform(self,X):#apply normlization on X_train or X_test\n",
    "        X_norm = (X-self.u)/(self.s+1e-6)## z = (x-u)/s, check: will self.s = 0 ?\n",
    "        return X_norm\n",
    "    \n",
    "def normalize_features(X_train,X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    return scaler.transform(X_train),scaler.transform(X_test)    \n",
    "## scoring function\n",
    "def accuracy(ypred, yexact):\n",
    "    p = np.array(ypred == yexact, dtype = int)\n",
    "    return np.sum(p)/float(len(yexact))\n",
    "## read dataset\n",
    "def read_dataset(feature_file,label_file):\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values #convert values in dataframe to numpy 2-D array\n",
    "    y = df_y.values.reshape(-1)   #convert values in dataframe to numpy 1-D array\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a9a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help functions for CNN.\n",
    "def to_tensor(X):\n",
    "    #2d array to 4d array\n",
    "    m_samples, n_features=X.shape\n",
    "    return X.reshape(m_samples,1,28,28)# Minist dataset features 874=28*28\n",
    "\n",
    "#convolutional operation\n",
    "def conv(x, k):\n",
    "    x_row, x_col = x.shape # data shape\n",
    "    k_row, k_col = k.shape # kernel shape \n",
    "    \n",
    "    result_row, result_col = x_row - k_row + 1, x_col - k_col + 1 #stride = 1\n",
    "    result = np.empty((result_row, result_col))\n",
    "    for i in range(result_row):\n",
    "        for j in range(result_col):\n",
    "            #summation of point-wise product of submatrix with kernel\n",
    "            sub = x[i : i + k_row, j : j + k_col]\n",
    "            result[i,j] = np.sum(sub * k)\n",
    "    return result\n",
    "\n",
    "#padding x with size1 rows/ size2 columns of zeros.\n",
    "def padding(x, size1,size2):\n",
    "    cur_r, cur_w = x.shape[0], x.shape[1]\n",
    "    new_r = cur_r + size1 * 2\n",
    "    new_w = cur_w + size2 * 2\n",
    "    result = np.zeros((new_r, new_w))\n",
    "    result[size1:cur_r + size1, size2:cur_w+size2] = x\n",
    "    return result\n",
    "\n",
    "def rot180(w):\n",
    "#rotate matrix for 180 degree:\n",
    "#array([[ 0,  1,  2,  3],\n",
    "#       [ 4,  5,  6,  7],\n",
    "#       [ 8,  9, 10, 11]])\n",
    "# to\n",
    "#array([[11, 10,  9,  8],\n",
    "#       [ 7,  6,  5,  4],\n",
    "#       [ 3,  2,  1,  0]])\n",
    "    m,n = w.shape \n",
    "    w180 = w.flatten()[::-1].reshape(m,n)\n",
    "    return w180\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368f401",
   "metadata": {},
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self,lr=0.001):\n",
    "        # in this example, in_channel = 1, out_channel = 2, kernel shape is (3,3)\n",
    "        self.w = np.random.randn(1, 2, 3, 3)/np.sqrt(3)\n",
    "        self.b = np.zeros((2)) \n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, in_data):      \n",
    "        self.input = in_data \n",
    "        \n",
    "        m_samples, in_channel, in_row, in_col = in_data.shape\n",
    "        out_channel = 2\n",
    "        # the kernel shape is (3,3)\n",
    "        out_row = in_row - 2\n",
    "        out_col = in_col - 2\n",
    "        \n",
    "        self.output = np.zeros((m_samples, out_channel, out_row, out_col))\n",
    "        #You have to iterate in_channel if in_channel>1.\n",
    "        for m in range(m_samples):\n",
    "            for o in range(out_channel):\n",
    "                self.output[m, o,:,:] += conv(in_data[m, 0,:,:], self.w[0, o,:,:])\n",
    "                self.output[m, o,:,:] += self.b[o]# add bias for each output\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,dz):\n",
    "        m_samples, out_channel, out_row, out_col = dz.shape #dz has the same shape as the output\n",
    "\n",
    "        # gradient of b has nothing to do with conv function; similar as ann's b term.\n",
    "        db = np.sum(dz,axis=(0,2,3))\n",
    "        \n",
    "        \n",
    "        #calculate dw to update current layers' gradient and dx for previous layer to update gradient\n",
    "        dw = np.zeros_like(self.w)\n",
    "        dx = np.zeros_like(self.input)\n",
    "        \n",
    "        #dL/dw  = sum_m con(x,dz). You have to iterate in_channel if in_channel>1.\n",
    "        for m in range(m_samples):\n",
    "            for o in range(out_channel):\n",
    "                dw[0, o,:,:] += conv(self.input[m,0,:,:], dz[m,o,:,:])\n",
    "                  \n",
    "        #dL/dx = sum_i conv(pad(dz),rot180(w)).You have to iterate in_channel if in_channel>1.\n",
    "        for m in range(m_samples):\n",
    "            for o in range(out_channel):\n",
    "                dz_padded = padding(dz[m,o,:,:],2,2)\n",
    "                w_rot = rot180(self.w[0,o,:,:])\n",
    "                dx[m, 0,  :,:] += conv(dz_padded,w_rot)\n",
    "\n",
    "        # update gradient\n",
    "        self.w = self.w - self.lr*dw\n",
    "        self.b = self.b - self.lr*db\n",
    "        ##return dx for previous layer to calculate gradient    \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1138ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, pooling = (2,2), strides = (2,2)):\n",
    "        self.pooling = pooling # \n",
    "        self.strides = strides \n",
    "\n",
    "    def forward(self, in_data):   \n",
    "        m_samples, in_channel, in_row, in_col = in_data.shape\n",
    "        s0 = self.strides[0]\n",
    "        s1 = self.strides[1]\n",
    "        out_row = (in_row - self.pooling[0]) // s0 + 1\n",
    "        out_col = (in_col - self.pooling[1]) // s1 + 1\n",
    "        \n",
    "        self.input = in_data\n",
    "        self.output = np.empty((m_samples, in_channel, out_row, out_col))\n",
    "        self.max_indices = np.zeros((m_samples, in_channel, out_row, out_col),dtype=tuple)\n",
    "\n",
    "        for m in np.arange(m_samples):\n",
    "            for i in np.arange(in_channel):\n",
    "                for r in np.arange(out_row):\n",
    "                    for c in np.arange(out_col):\n",
    "                        region = in_data[m, i, r * s0 : r*s0 + self.pooling[0], c*s1 : c*s1 + self.pooling[1]]\n",
    "                        self.output[m, i, r, c] = np.max(region)\n",
    "                        # this is the max indices of the region [n, i, j]. it is a np array\n",
    "                        self.max_indices[m, i, r, c] = np.unravel_index(region.argmax(), region.shape)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dz):\n",
    "        m_samples,in_channel,in_row, in_col = self.input.shape\n",
    "        _, out_channel, out_row, out_col = self.output.shape\n",
    "        \n",
    "        dx = np.zeros_like(self.input)\n",
    "\n",
    "        for m in np.arange(m_samples):\n",
    "            for i in np.arange(in_channel):\n",
    "                for r in np.arange(out_row):\n",
    "                    for c in np.arange(out_col):\n",
    "                        dx[m, i, self.max_indices[m, i, r, c][0]+r, self.max_indices[m, i, r, c][1]+c] = dz[m, i, r, c]\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ba81023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, in_data):\n",
    "        self.m_samples, self.in_channel, self.in_row, self.in_col = in_data.shape\n",
    "        return in_data.reshape(self.m_samples, self.in_channel * self.in_row * self.in_col)\n",
    "    def backward(self, dz):\n",
    "        return dz.reshape(self.m_samples, self.in_channel, self.in_row, self.in_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2685f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, in_num, out_num, lr = 0.001):\n",
    "        self.in_num = in_num\n",
    "        self.out_num = out_num\n",
    "        self.w = np.random.randn(in_num, out_num)/np.sqrt(in_num)\n",
    "        self.b = np.zeros((1,out_num))\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        self.input = in_data\n",
    "        self.output = np.dot(in_data,self.w) + self.b\n",
    "        return self.output\n",
    "    def backward(self, dz):\n",
    "        #similar as hidden layer in ann, but note that we do not include activation or softmax function in FCLayer.\n",
    "        dw = np.dot(self.input.T, dz) \n",
    "        db = np.sum(dz,axis=0, keepdims=True)\n",
    "        dx = dz.dot(self.w.T)\n",
    "        self.w -= self.lr * dw\n",
    "        self.b -= self.lr * db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4040ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class relulayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,in_data):\n",
    "        # relu(x)= x if x>0 ; 0 otherwise\n",
    "        self.input = in_data\n",
    "        self.output = in_data.copy()\n",
    "        self.output[self.output<0] = 0\n",
    "        return self.output\n",
    "    def backward(self,dz):\n",
    "        dx = dz.copy()\n",
    "        dx[self.input<0]=0\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "789e57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def softmax(self,z):\n",
    "        exp_value = np.exp(z-np.amax(z, axis=1, keepdims=True)) # for stablility\n",
    "        # keepdims = True means that the output's dimension is the same as of z\n",
    "        softmax_scores = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "        return softmax_scores\n",
    "    \n",
    "    def forward(self, in_data):\n",
    "        self.input = in_data\n",
    "        self.output = self.softmax(in_data)\n",
    "        return self.output\n",
    "    def backward(self, y):\n",
    "        return self.output - y #y_hat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71fc5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.y_prob = None\n",
    "        self.loss = None\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_data(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def feed_forward(self):                # feed forward \n",
    "        input = self.X\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        self.y_prob = input\n",
    "        return input\n",
    "    \n",
    "    def back_propagation(self):                # back propagate\n",
    "        dz = self.y # for the output layer, the input gradient is indeed target.\n",
    "        for layer in reversed(self.layers):\n",
    "            dz = layer.backward(dz) \n",
    "    \n",
    "    def cross_entropy_loss(self):\n",
    "        #  $L = -\\sum_n\\sum_{i\\in C} y_{n, i}\\log(\\hat{y}_{n, i})$\n",
    "        # calculate y_hat\n",
    "        self.feed_forward()\n",
    "        self.loss = -np.sum(self.y*np.log(self.y_prob + 1e-6)) \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        input = X_test\n",
    "        for layer in self.layers:\n",
    "            #if plot_feature_maps:\n",
    "            #    image = (image * 255)[0, :, :]\n",
    "            #    plot_sample(image, None, None)\n",
    "            input = layer.forward(input)\n",
    "        return np.argmax(input, axis=1)   \n",
    "    \n",
    "    def mini_batch_descent(self, X, y, batch_size=50):\n",
    "        \"\"\"\n",
    "            Note that y is a 2d array. \n",
    "        \"\"\"\n",
    "        shuffled_idx = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffled_idx)\n",
    "        n_batches = X.shape[0] // batch_size \n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            self.feed_data(X[shuffled_idx[i*batch_size:(i+1)*batch_size]], y[shuffled_idx[i*batch_size:(i+1)*batch_size]])\n",
    "            self.feed_forward()\n",
    "            self.back_propagation()\n",
    "        if X.shape[0] % batch_size != 0:\n",
    "            self.feed_data(X[shuffled_idx[n_batches*batch_size:]], y[shuffled_idx[n_batches*batch_size:]])\n",
    "            self.feed_forward()\n",
    "            self.back_propagation()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91d2ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, current loss = 119.67, test accuracy = 64.60%\n",
      "epoch = 2, current loss = 73.86, test accuracy = 73.20%\n",
      "epoch = 3, current loss = 53.43, test accuracy = 77.80%\n",
      "epoch = 4, current loss = 47.94, test accuracy = 80.00%\n",
      "epoch = 5, current loss = 51.05, test accuracy = 82.00%\n",
      "epoch = 6, current loss = 44.69, test accuracy = 83.20%\n",
      "epoch = 7, current loss = 41.15, test accuracy = 84.00%\n",
      "epoch = 8, current loss = 33.23, test accuracy = 84.40%\n",
      "epoch = 9, current loss = 39.08, test accuracy = 85.00%\n",
      "epoch = 10, current loss = 23.49, test accuracy = 86.40%\n",
      "epoch = 11, current loss = 28.82, test accuracy = 87.20%\n",
      "epoch = 12, current loss = 22.16, test accuracy = 87.40%\n",
      "epoch = 13, current loss = 21.75, test accuracy = 88.00%\n",
      "epoch = 14, current loss = 27.73, test accuracy = 87.60%\n",
      "epoch = 15, current loss = 26.49, test accuracy = 87.80%\n",
      "epoch = 16, current loss = 25.66, test accuracy = 88.20%\n",
      "epoch = 17, current loss = 18.35, test accuracy = 87.40%\n",
      "epoch = 18, current loss = 29.53, test accuracy = 88.40%\n",
      "epoch = 19, current loss = 24.83, test accuracy = 87.40%\n",
      "epoch = 20, current loss = 39.49, test accuracy = 88.60%\n",
      "epoch = 21, current loss = 21.07, test accuracy = 88.60%\n",
      "epoch = 22, current loss = 17.86, test accuracy = 89.00%\n",
      "epoch = 23, current loss = 15.76, test accuracy = 88.80%\n",
      "epoch = 24, current loss = 17.45, test accuracy = 89.00%\n",
      "epoch = 25, current loss = 15.57, test accuracy = 89.00%\n",
      "epoch = 26, current loss = 16.62, test accuracy = 88.60%\n",
      "epoch = 27, current loss = 14.01, test accuracy = 89.40%\n",
      "epoch = 28, current loss = 15.94, test accuracy = 89.80%\n",
      "epoch = 29, current loss = 22.56, test accuracy = 89.40%\n",
      "epoch = 30, current loss = 15.58, test accuracy = 89.60%\n",
      "epoch = 31, current loss = 18.38, test accuracy = 89.40%\n",
      "epoch = 32, current loss = 14.99, test accuracy = 90.00%\n",
      "epoch = 33, current loss = 15.87, test accuracy = 90.20%\n",
      "epoch = 34, current loss = 13.00, test accuracy = 89.80%\n",
      "epoch = 35, current loss = 11.71, test accuracy = 89.80%\n",
      "epoch = 36, current loss = 8.92, test accuracy = 90.00%\n",
      "epoch = 37, current loss = 11.16, test accuracy = 90.40%\n",
      "epoch = 38, current loss = 12.50, test accuracy = 90.00%\n",
      "epoch = 39, current loss = 16.77, test accuracy = 90.00%\n",
      "epoch = 40, current loss = 6.85, test accuracy = 90.00%\n",
      "Accuracy of my CNN model is 90.00%\n",
      "Takes 1779.43s\n"
     ]
    }
   ],
   "source": [
    "import time        \n",
    "# load data\n",
    "X_train, y_train = read_dataset('MNIST_X_train.csv', 'MNIST_y_train.csv')\n",
    "X_test, y_test = read_dataset('MNIST_X_test.csv', 'MNIST_y_test.csv')\n",
    "#normlize featurues / encode labels / convert features into one-hot-vector.\n",
    "X_train_norm, X_test_norm = normalize_features(X_train, X_test)\n",
    "y_train_ohe, y_test_ohe = ohe(y_train, y_test)\n",
    "X_train_tensor,X_test_tensor = to_tensor(X_train_norm),to_tensor(X_test_norm)\n",
    "# initialize network\n",
    "myCNN = Network() \n",
    "myCNN.add_layer(ConvLayer(lr=0.0001))\n",
    "myCNN.add_layer(MaxPoolingLayer())\n",
    "#conv;pad; for your homework\n",
    "myCNN.add_layer(FlattenLayer())\n",
    "myCNN.add_layer(LinearLayer(in_num=338, out_num=100,lr=0.0001))\n",
    "myCNN.add_layer(relulayer())\n",
    "myCNN.add_layer(LinearLayer(in_num=100, out_num = y_train_ohe.shape[1],lr=0.0001))\n",
    "myCNN.add_layer(SoftmaxLayer())\n",
    "#start training\n",
    "start = time.time()\n",
    "epoch_num = 40\n",
    "for i in range(epoch_num):\n",
    "    myCNN.mini_batch_descent(X_train_tensor, y_train_ohe, batch_size=100)\n",
    "    myCNN.cross_entropy_loss()\n",
    "    y_pred = myCNN.predict(X_test_tensor)\n",
    "    print('epoch = {}, current loss = {:.2f}, test accuracy = {:.2f}%'.format(i+1, myCNN.loss, 100*accuracy(y_pred, y_test.ravel())))      \n",
    "    \n",
    "end = time.time()\n",
    "y_pred = myCNN.predict(X_test_tensor)\n",
    "print('Accuracy of my CNN model is {:.2f}%'.format(accuracy(y_pred, y_test.ravel())*100))\n",
    "print('Takes {:.2f}s'.format(end-start))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec099f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
