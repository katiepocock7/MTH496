{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ada3a5",
   "metadata": {},
   "source": [
    "# MTH 496 HW3 \n",
    "\n",
    "Due date: 5pm, Nov 29, 2022\n",
    "\n",
    "Katie Pocock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "86870baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math as mth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c973a59",
   "metadata": {},
   "source": [
    "**&#9989; **Problem 1 (40pts)****.  The Digits datasets are given. Classify this dataset using ANN models with different layer configurations. Note that the layer configuration must be an adjustable parameter: the number of layers and the number of neurons in each layer can be chosen arbitrarily (within reasonable limits). Present your results with the following layer configurations:\n",
    "- One hidden layer with 200 neurons.\n",
    "- Two hidden layers: each has 100 neur\\ons.\n",
    "- Five hidden layers with 50,100,100,100,50 neurons, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf39e69",
   "metadata": {},
   "source": [
    "Two hidden layers : (did it the first by basing it off of the ann code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2586e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X_train): ##fit X_train, calculate mean, std of X_train\n",
    "        self.u = X_train.mean(axis=0)\n",
    "        self.s = X_train.std(axis=0)\n",
    "    def transform(self,X):#apply normlization on X_train or X_test\n",
    "        X_norm = (X-self.u)/(self.s+1e-6)## z = (x-u)/s, check: will self.s = 0 ?\n",
    "        return X_norm\n",
    "    \n",
    "class one_hot_encoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,y_train):\n",
    "    #fit y_train, find set{y_train} and assign elements of this set with one-hot-vectors.\n",
    "       label_set = set(y_train)\n",
    "    #assign index to labels:\n",
    "       self.label_list = list(label_set)\n",
    "    \n",
    "    def transform(self,y): #apply one-hot-encoding to y_train or y_test\n",
    "        #the shape of output matrix is (y.shape[0],len(self.label_list))\n",
    "        ohe_matrix = np.zeros((y.shape[0],len(self.label_list)))\n",
    "        for label,vector in zip(y,ohe_matrix):\n",
    "            #find index of label and add 1 to vector[index]\n",
    "            index = self.label_list.index(label)\n",
    "            vector[index]+=1\n",
    "        return ohe_matrix.astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "def read_dataset(feature_file,label_file):\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values #convert values in dataframe to numpy 2-D array\n",
    "    y = df_y.values.reshape(-1)   #convert values in dataframe to numpy 1-D array\n",
    "    return X,y\n",
    "\n",
    "\n",
    "\n",
    "def normalize_features(X_train,X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    return scaler.transform(X_train),scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ohe(y_train,y_test):\n",
    "    ohencoder = one_hot_encoder()\n",
    "    ohencoder.fit(y_train)\n",
    "    return ohencoder.transform(y_train),ohencoder.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    exp_value = np.exp(z-np.amax(z, axis=1, keepdims=True)) # for stablility\n",
    "    # keepdims = True means that the output's dimension is the same as of z\n",
    "    softmax_scores = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "    return softmax_scores\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(ypred, yexact):\n",
    "    p = np.array(ypred == yexact, dtype = int)\n",
    "    return np.sum(p)/float(len(yexact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c6cd970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class twolayer_NN:\n",
    "    def __init__(self, X, y, hidden_layer_nn_0=100, hidden_layer_nn_1=100, lr=0.01):\n",
    "        self.X = X # features\n",
    "        self.y = y # labels (targets) in one-hot-encoder\n",
    "        self.lr = lr # learning rate\n",
    "        # Initialize weights\n",
    "        self.nn = X.shape[1] # number of neurons in the input layer\n",
    "\n",
    "        self.Wp = np.random.randn(self.nn, hidden_layer_nn_0) / np.sqrt(self.nn)\n",
    "        self.bp = np.zeros((1, hidden_layer_nn_0)) \n",
    "        \n",
    "        self.W0 = np.random.randn(hidden_layer_nn_0, hidden_layer_nn_1) / np.sqrt(hidden_layer_nn_0)\n",
    "        self.b0 = np.zeros((1, hidden_layer_nn_1)) \n",
    "        \n",
    "        self.output_layer_nn = y.shape[1]\n",
    "        self.Wq = np.random.randn(hidden_layer_nn_1, self.output_layer_nn) / np.sqrt(hidden_layer_nn_1)\n",
    "        self.bq = np.zeros((1, self.output_layer_nn))      \n",
    "        \n",
    "    def feed_forward(self):\n",
    "        # hidden layer\n",
    "        ## z_0 = XW_p + b_p\n",
    "        self.z0 = np.dot(self.X, self.Wp) + self.bp\n",
    "        ## activation function :  f_0 = \\tanh(z_0)\n",
    "        self.f0 = np.tanh(self.z0)\n",
    "        ## z_1 = f_0W_0 + b_0\n",
    "        self.z1 = np.dot(self.f0, self.W0) + self.b0\n",
    "        ## activation function :  f_1 = \\tanh(z_1)\n",
    "        self.f1 = np.tanh(self.z1)\n",
    "        \n",
    "        # output layer\n",
    "        ## z_q = f_1W_q + b_q\n",
    "        self.zq = np.dot(self.f1, self.Wq) + self.bq   \n",
    "        #\\hat{y} = softmax}(z_q)$\n",
    "        self.y_hat = softmax(self.zq)\n",
    "        \n",
    "    def back_propagation(self):\n",
    "        # $d_3 = \\hat{y}-y$\n",
    "        dq = self.y_hat - self.y\n",
    "        # d_2 = (1-f^2_2)*(\\hat{y}-y)W_3^T\n",
    "        d0 = (1-self.f1*self.f1)*(dq.dot((self.Wq).T))\n",
    "        dp = (1-self.f0*self.f0)*(d0.dot((self.W0).T))\n",
    "        \n",
    "        # dL/dWq = f_1^T d3\n",
    "        dWq = np.dot(self.f1.T, dq)\n",
    "        # dL/dbq = sum(dq,axis=0)\n",
    "        dbq = np.sum(dq, axis=0, keepdims=True)\n",
    "        \n",
    "        # dL/dW0 = f_0^T d_0\n",
    "        dW0 = np.dot(self.f0.T, d0)\n",
    "        # dL/b_0 = sum(d_0,axis=0)\n",
    "        db0 = np.sum(d0, axis=0, keepdims=True)\n",
    "        # axis =0 : sum along the vertical axis\n",
    "\n",
    "        # dL/dW_1} = x^T d_1\n",
    "        dWp = np.dot((self.X).T, dp)\n",
    "        # dL/db_1 = d_1.axis=0\n",
    "        dbp = np.sum(dp, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update the gradident descent\n",
    "        self.Wp = self.Wp - self.lr * dWp\n",
    "        self.bp = self.bp - self.lr * dbp\n",
    "        self.W0 = self.W0 - self.lr * dW0\n",
    "        self.b0 = self.b0 - self.lr * db0\n",
    "        self.Wq = self.Wq - self.lr * dWq\n",
    "        self.bq = self.bq - self.lr * dbq\n",
    "        \n",
    "    def cross_entropy_loss(self):\n",
    "        #  $L = -\\sum_n\\sum_{i\\in C} y_{n, i}\\log(\\hat{y}_{n, i})$\n",
    "        # calculate y_hat\n",
    "        self.feed_forward()\n",
    "        self.loss = -np.sum(self.y*np.log(self.y_hat + 1e-6))\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        # Use feed forward to calculat y_hat_test\n",
    "        # hidden layer\n",
    "        ## z_p = XW_p + b_p\n",
    "        z0 = np.dot(X_test, self.Wp) + self.bp\n",
    "        ## activation function :  f_0 = \\tanh(z_0)\n",
    "        f0 = np.tanh(z0)\n",
    "        ## z_1 = f_0W_0 + b_0\n",
    "        z1 = np.dot(f0, self.W0) + self.b0\n",
    "        f1 = np.tanh(z1)\n",
    "        # output layer\n",
    "        ## z_q = f_1W_q + b_q\n",
    "        zq = np.dot(f1, self.Wq) + self.bq    \n",
    "        #\\hat{y} = softmax(z_q)$\n",
    "        y_hat_test = softmax(zq)\n",
    "        # the rest is similar to the logistic regression\n",
    "        labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        num_test_samples = X_test.shape[0]\n",
    "        # find which index gives us the highest probability\n",
    "        ypred = np.zeros(num_test_samples, dtype=int) \n",
    "        for i in range(num_test_samples):\n",
    "            ypred[i] = labels[np.argmax(y_hat_test[i,:])]\n",
    "        return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a62a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_dataset('Digits_X_train.csv', 'Digits_y_train.csv')\n",
    "X_test, y_test = read_dataset('Digits_X_test.csv', 'Digits_y_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0c502a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20, current loss = 149.46214\n",
      "epoch = 40, current loss = 42.32578\n",
      "epoch = 60, current loss = 20.79385\n",
      "epoch = 80, current loss = 13.12941\n",
      "epoch = 100, current loss = 9.39530\n",
      "epoch = 120, current loss = 7.23351\n",
      "epoch = 140, current loss = 5.83950\n",
      "epoch = 160, current loss = 4.87256\n",
      "epoch = 180, current loss = 4.16581\n",
      "epoch = 200, current loss = 3.62850\n",
      "Accuracy of our model  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-120-33d6251bed83>:65: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  p = np.array(ypred == yexact, dtype = int)\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "my2NN = twolayer_NN(X_train_norm, y_train_ohe, hidden_layer_nn_0=300, hidden_layer_nn_1=100, lr=0.001)  \n",
    "epoch_num = 200\n",
    "for i in range(epoch_num):\n",
    "    my2NN.feed_forward()\n",
    "    my2NN.back_propagation()\n",
    "    my2NN.cross_entropy_loss()\n",
    "    if ((i+1)%20 == 0):\n",
    "        print('epoch = %d, current loss = %.5f' % (i+1, my2NN.loss))         \n",
    "        \n",
    "y_pred = my2NN.predict(X_test_norm)\n",
    "print('Accuracy of our model ', accuracy(y_pred, y_test.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "92f1b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help classes and functions (same as ANN).\n",
    "##one-hot-encoding\n",
    "class one_hot_encoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,y_train):\n",
    "    #fit y_train, find set{y_train} and assign elements of this set with one-hot-vectors.\n",
    "       label_set = set(y_train)\n",
    "    #assign index to labels:\n",
    "       self.label_list = list(label_set)\n",
    "    \n",
    "    def transform(self,y): #apply one-hot-encoding to y_train or y_test\n",
    "        #the shape of output matrix is (y.shape[0],len(self.label_list))\n",
    "        ohe_matrix = np.zeros((y.shape[0],len(self.label_list)))\n",
    "        for label,vector in zip(y,ohe_matrix):\n",
    "            #find index of label and add 1 to vector[index]\n",
    "            index = self.label_list.index(label)\n",
    "            vector[index]+=1\n",
    "        return ohe_matrix.astype(int)\n",
    "    \n",
    "def ohe(y_train,y_test):\n",
    "    ohencoder = one_hot_encoder()\n",
    "    ohencoder.fit(y_train)\n",
    "    return ohencoder.transform(y_train),ohencoder.transform(y_test)\n",
    "### normalize feature\n",
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X_train): ##fit X_train, calculate mean, std of X_train\n",
    "        self.u = X_train.mean(axis=0)\n",
    "        self.s = X_train.std(axis=0)\n",
    "    def transform(self,X):#apply normlization on X_train or X_test\n",
    "        X_norm = (X-self.u)/(self.s+1e-6)## z = (x-u)/s, check: will self.s = 0 ?\n",
    "        return X_norm\n",
    "    \n",
    "def normalize_features(X_train,X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    return scaler.transform(X_train),scaler.transform(X_test)    \n",
    "## scoring function\n",
    "def accuracy(ypred, yexact):\n",
    "    p = np.array(ypred == yexact, dtype = int)\n",
    "    return np.sum(p)/float(len(yexact))\n",
    "## read dataset\n",
    "def read_dataset(feature_file,label_file):\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values #convert values in dataframe to numpy 2-D array\n",
    "    y = df_y.values.reshape(-1)   #convert values in dataframe to numpy 1-D array\n",
    "    return X,y\n",
    "\n",
    "def to_tensor(X):\n",
    "    #2d array to 4d array\n",
    "    m_samples, n_features=X.shape\n",
    "    return X.reshape(m_samples,1,int(np.sqrt(n_features)),int(np.sqrt(n_features)))# Minist dataset features 874=28*28\n",
    "\n",
    "#convolutional operation\n",
    "def conv(x, k):\n",
    "    x_row, x_col = x.shape # data shape\n",
    "    k_row, k_col = k.shape # kernel shape \n",
    "    \n",
    "    result_row, result_col = x_row - k_row + 1, x_col - k_col + 1 #stride = 1\n",
    "    result = np.empty((result_row, result_col))\n",
    "    for i in range(result_row):\n",
    "        for j in range(result_col):\n",
    "            #summation of point-wise product of submatrix with kernel\n",
    "            sub = x[i : i + k_row, j : j + k_col]\n",
    "            result[i,j] = np.sum(sub * k)\n",
    "    return result\n",
    "\n",
    "#padding x with size1 rows/ size2 columns of zeros.\n",
    "def padding(x, size1,size2):\n",
    "    cur_r, cur_w = x.shape[0], x.shape[1]\n",
    "    new_r = cur_r + size1 * 2\n",
    "    new_w = cur_w + size2 * 2\n",
    "    result = np.zeros((new_r, new_w))\n",
    "    result[size1:cur_r + size1, size2:cur_w+size2] = x\n",
    "    return result\n",
    "\n",
    "def rot180(w):\n",
    "#rotate matrix for 180 degree:\n",
    "#array([[ 0,  1,  2,  3],\n",
    "#       [ 4,  5,  6,  7],\n",
    "#       [ 8,  9, 10, 11]])\n",
    "# to\n",
    "#array([[11, 10,  9,  8],\n",
    "#       [ 7,  6,  5,  4],\n",
    "#       [ 3,  2,  1,  0]])\n",
    "    m,n = w.shape \n",
    "    w180 = w.flatten()[::-1].reshape(m,n)\n",
    "    return w180\n",
    "\n",
    "class ConvLayer1:\n",
    "    def __init__(self,lr=0.001):\n",
    "        # in this example, in_channel = 1, out_channel = 2, kernel shape is (3,3)\n",
    "        self.w = np.random.randn(1, 1, 5,5)/np.sqrt(5)\n",
    "        self.b = np.zeros((2)) \n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, in_data):      \n",
    "        self.input = in_data \n",
    "        \n",
    "        m_samples, in_channel, in_row, in_col = in_data.shape\n",
    "        out_channel = 1\n",
    "        # the kernel shape is (5,5)\n",
    "        out_row = in_row - 4\n",
    "        out_col = in_col - 4\n",
    "        \n",
    "        self.output = np.zeros((m_samples, out_channel, out_row, out_col))\n",
    "        #You have to iterate in_channel if in_channel>1.\n",
    "        for m in range(m_samples):\n",
    "            for o in range(out_channel):\n",
    "                self.output[m, o,:,:] += conv(in_data[m, 0,:,:], self.w[0, o,:,:])\n",
    "                self.output[m, o,:,:] += self.b[o]# add bias for each output\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,dz):\n",
    "        m_samples, out_channel, out_row, out_col = dz.shape #dz has the same shape as the output\n",
    "\n",
    "        # gradient of b has nothing to do with conv function; similar as ann's b term.\n",
    "        db = np.sum(dz,axis=(0,2,3))\n",
    "        \n",
    "        \n",
    "        #calculate dw to update current layers' gradient and dx for previous layer to update gradient\n",
    "        dw = np.zeros_like(self.w)\n",
    "        dx = np.zeros_like(self.input)\n",
    "        \n",
    "        #dL/dw  = sum_m con(x,dz). You have to iterate in_channel if in_channel>1.\n",
    "        for m in range(m_samples):\n",
    "            for o in range(out_channel):\n",
    "                dw[0, o,:,:] += conv(self.input[m,0,:,:], dz[m,o,:,:])\n",
    "                  \n",
    "        #dL/dx = sum_i conv(pad(dz),rot180(w)).You have to iterate in_channel if in_channel>1.\n",
    "        for m in range(m_samples):\n",
    "            for o in range(out_channel):\n",
    "                dz_padded = padding(dz[m,o,:,:],4,4)\n",
    "                w_rot = rot180(self.w[0,o,:,:])\n",
    "                dx[m, 0,  :,:] += conv(dz_padded,w_rot)\n",
    "\n",
    "        # update gradient\n",
    "        self.w = self.w - self.lr*dw\n",
    "        self.b = self.b - self.lr*db\n",
    "        ##return dx for previous layer to calculate gradient    \n",
    "        return dx\n",
    "  \n",
    "\n",
    "class ConvLayer2:\n",
    "    def __init__(self,lr=0.001):\n",
    "        # in this example, in_channel = 1, out_channel = 2, kernel shape is (3,3)\n",
    "        self.w = np.random.randn(1, 2, 3, 3)/np.sqrt(3)\n",
    "        self.b = np.zeros((2)) \n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, in_data):      \n",
    "        self.input = in_data \n",
    "        \n",
    "        m_samples, in_channel, in_row, in_col = in_data.shape\n",
    "        out_channel = 2\n",
    "        # the kernel shape is (3,3)\n",
    "        out_row = in_row - 2\n",
    "        out_col = in_col - 2\n",
    "        \n",
    "        self.output = np.zeros((m_samples, out_channel, out_row, out_col))\n",
    "        #You have to iterate in_channel if in_channel>1.\n",
    "        for m in range(m_samples):\n",
    "            for o in range(out_channel):\n",
    "                self.output[m, o,:,:] += conv(in_data[m, 0,:,:], self.w[0, o,:,:])\n",
    "                self.output[m, o,:,:] += self.b[o]# add bias for each output\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,dz):\n",
    "        m_samples, out_channel, out_row, out_col = dz.shape #dz has the same shape as the output\n",
    "\n",
    "        # gradient of b has nothing to do with conv function; similar as ann's b term.\n",
    "        db = np.sum(dz,axis=(0,2,3))\n",
    "        \n",
    "        \n",
    "        #calculate dw to update current layers' gradient and dx for previous layer to update gradient\n",
    "        dw = np.zeros_like(self.w)\n",
    "        dx = np.zeros_like(self.input)\n",
    "        \n",
    "        #dL/dw  = sum_m con(x,dz). You have to iterate in_channel if in_channel>1.\n",
    "        for m in range(m_samples):\n",
    "            for o in range(out_channel):\n",
    "                dw[0, o,:,:] += conv(self.input[m,0,:,:], dz[m,o,:,:])\n",
    "                  \n",
    "        #dL/dx = sum_i conv(pad(dz),rot180(w)).You have to iterate in_channel if in_channel>1.\n",
    "        for m in range(m_samples):\n",
    "            for o in range(out_channel):\n",
    "                dz_padded = padding(dz[m,o,:,:],2,2)\n",
    "                w_rot = rot180(self.w[0,o,:,:])\n",
    "                dx[m, 0,  :,:] += conv(dz_padded,w_rot)\n",
    "\n",
    "        # update gradient\n",
    "        self.w = self.w - self.lr*dw\n",
    "        self.b = self.b - self.lr*db\n",
    "        ##return dx for previous layer to calculate gradient    \n",
    "        return dx\n",
    "    \n",
    "\n",
    "    \n",
    "class MaxPoolingLayer:\n",
    "    def __init__(self, pooling = (2,2), strides = (2,2)):\n",
    "        self.pooling = pooling # \n",
    "        self.strides = strides \n",
    "\n",
    "    def forward(self, in_data):   \n",
    "        m_samples, in_channel, in_row, in_col = in_data.shape\n",
    "        s0 = self.strides[0]\n",
    "        s1 = self.strides[1]\n",
    "        out_row = (in_row - self.pooling[0]) // s0 + 1\n",
    "        out_col = (in_col - self.pooling[1]) // s1 + 1\n",
    "        \n",
    "        self.input = in_data\n",
    "        self.output = np.empty((m_samples, in_channel, out_row, out_col))\n",
    "        self.max_indices = np.zeros((m_samples, in_channel, out_row, out_col),dtype=tuple)\n",
    "\n",
    "        for m in np.arange(m_samples):\n",
    "            for i in np.arange(in_channel):\n",
    "                for r in np.arange(out_row):\n",
    "                    for c in np.arange(out_col):\n",
    "                        region = in_data[m, i, r * s0 : r*s0 + self.pooling[0], c*s1 : c*s1 + self.pooling[1]]\n",
    "                        self.output[m, i, r, c] = np.max(region)\n",
    "                        # this is the max indices of the region [n, i, j]. it is a np array\n",
    "                        self.max_indices[m, i, r, c] = np.unravel_index(region.argmax(), region.shape)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dz):\n",
    "        m_samples,in_channel,in_row, in_col = self.input.shape\n",
    "        _, out_channel, out_row, out_col = self.output.shape\n",
    "        \n",
    "        dx = np.zeros_like(self.input)\n",
    "\n",
    "        for m in np.arange(m_samples):\n",
    "            for i in np.arange(in_channel):\n",
    "                for r in np.arange(out_row):\n",
    "                    for c in np.arange(out_col):\n",
    "                        dx[m, i, self.max_indices[m, i, r, c][0]+r, self.max_indices[m, i, r, c][1]+c] = dz[m, i, r, c]\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, in_data):\n",
    "        self.m_samples, self.in_channel, self.in_row, self.in_col = in_data.shape\n",
    "        return in_data.reshape(self.m_samples, self.in_channel * self.in_row * self.in_col)\n",
    "    def backward(self, dz):\n",
    "        return dz.reshape(self.m_samples, self.in_channel, self.in_row, self.in_col)    \n",
    "    \n",
    "class LinearLayer:\n",
    "    def __init__(self, in_num, out_num, lr = 0.001):\n",
    "        self.in_num = in_num\n",
    "        self.out_num = out_num\n",
    "        self.w = np.random.randn(in_num, out_num)/np.sqrt(in_num)\n",
    "        self.b = np.zeros((1,out_num))\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        self.input = in_data\n",
    "        self.output = np.dot(in_data,self.w) + self.b\n",
    "        return self.output\n",
    "    def backward(self, dz):\n",
    "        #similar as hidden layer in ann, but note that we do not include activation or softmax function in FCLayer.\n",
    "        dw = np.dot(self.input.T, dz) \n",
    "        db = np.sum(dz,axis=0, keepdims=True)\n",
    "        dx = dz.dot(self.w.T)\n",
    "        self.w -= self.lr * dw\n",
    "        self.b -= self.lr * db\n",
    "        return dx  \n",
    "    \n",
    "    \n",
    "class relulayer: #activation function \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,in_data):\n",
    "        # relu(x)= x if x>0 ; 0 otherwise\n",
    "        self.input = in_data\n",
    "        self.output = in_data.copy()\n",
    "        self.output[self.output<0] = 0\n",
    "        return self.output\n",
    "    def backward(self,dz):\n",
    "        dx = dz.copy()\n",
    "        dx[self.input<0]=0\n",
    "        return dx    \n",
    "    \n",
    "    \n",
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def softmax(self,z):\n",
    "        exp_value = np.exp(z-np.amax(z, axis=1, keepdims=True)) # for stablility\n",
    "        # keepdims = True means that the output's dimension is the same as of z\n",
    "        softmax_scores = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "        return softmax_scores\n",
    "    \n",
    "    def forward(self, in_data):\n",
    "        self.input = in_data\n",
    "        self.output = self.softmax(in_data)\n",
    "        return self.output\n",
    "    def backward(self, y):\n",
    "        return self.output - y #y_hat - y\n",
    "    \n",
    "    \n",
    "class Network():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.y_prob = None\n",
    "        self.loss = None\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_data(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def feed_forward(self):                # feed forward \n",
    "        input = self.X\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        self.y_prob = input\n",
    "        return input\n",
    "    \n",
    "    def back_propagation(self):                # back propagate\n",
    "        dz = self.y # for the output layer, the input gradient is indeed target.\n",
    "        for layer in reversed(self.layers):\n",
    "            dz = layer.backward(dz) \n",
    "    \n",
    "    def cross_entropy_loss(self):\n",
    "        #  $L = -\\sum_n\\sum_{i\\in C} y_{n, i}\\log(\\hat{y}_{n, i})$\n",
    "        # calculate y_hat\n",
    "        self.feed_forward()\n",
    "        self.loss = -np.sum(self.y*np.log(self.y_prob + 1e-6)) \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        input = X_test\n",
    "        for layer in self.layers:\n",
    "            #if plot_feature_maps:\n",
    "            #    image = (image * 255)[0, :, :]\n",
    "            #    plot_sample(image, None, None)\n",
    "            input = layer.forward(input)\n",
    "        return np.argmax(input, axis=1)   \n",
    "    \n",
    "    def mini_batch_descent(self, X, y, batch_size=50):\n",
    "        \"\"\"\n",
    "            Note that y is a 2d array. \n",
    "        \"\"\"\n",
    "        shuffled_idx = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffled_idx)\n",
    "        n_batches = X.shape[0] // batch_size \n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            self.feed_data(X[shuffled_idx[i*batch_size:(i+1)*batch_size]], y[shuffled_idx[i*batch_size:(i+1)*batch_size]])\n",
    "            self.feed_forward()\n",
    "            self.back_propagation()\n",
    "        if X.shape[0] % batch_size != 0:\n",
    "            self.feed_data(X[shuffled_idx[n_batches*batch_size:]], y[shuffled_idx[n_batches*batch_size:]])\n",
    "            self.feed_forward()\n",
    "            self.back_propagation()        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c7cc8906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, current loss = 84.61, test accuracy = 85.11%\n",
      "epoch = 2, current loss = 39.40, test accuracy = 91.33%\n",
      "epoch = 3, current loss = 34.49, test accuracy = 92.22%\n",
      "epoch = 4, current loss = 24.95, test accuracy = 92.44%\n",
      "epoch = 5, current loss = 27.18, test accuracy = 92.89%\n",
      "epoch = 6, current loss = 25.99, test accuracy = 93.78%\n",
      "epoch = 7, current loss = 13.32, test accuracy = 94.00%\n",
      "epoch = 8, current loss = 17.97, test accuracy = 94.22%\n",
      "epoch = 9, current loss = 19.32, test accuracy = 94.00%\n",
      "epoch = 10, current loss = 20.91, test accuracy = 94.44%\n",
      "epoch = 11, current loss = 18.67, test accuracy = 94.67%\n",
      "epoch = 12, current loss = 17.01, test accuracy = 94.67%\n",
      "epoch = 13, current loss = 8.10, test accuracy = 94.44%\n",
      "epoch = 14, current loss = 9.82, test accuracy = 94.89%\n",
      "epoch = 15, current loss = 12.29, test accuracy = 94.67%\n",
      "epoch = 16, current loss = 6.50, test accuracy = 94.44%\n",
      "epoch = 17, current loss = 13.36, test accuracy = 94.44%\n",
      "epoch = 18, current loss = 6.12, test accuracy = 94.67%\n",
      "epoch = 19, current loss = 8.17, test accuracy = 94.67%\n",
      "epoch = 20, current loss = 14.55, test accuracy = 94.67%\n",
      "epoch = 21, current loss = 6.02, test accuracy = 94.67%\n",
      "epoch = 22, current loss = 8.04, test accuracy = 94.67%\n",
      "epoch = 23, current loss = 5.69, test accuracy = 94.44%\n",
      "epoch = 24, current loss = 8.37, test accuracy = 94.44%\n",
      "epoch = 25, current loss = 3.69, test accuracy = 94.67%\n",
      "epoch = 26, current loss = 8.41, test accuracy = 94.67%\n",
      "epoch = 27, current loss = 8.05, test accuracy = 94.67%\n",
      "epoch = 28, current loss = 7.49, test accuracy = 94.67%\n",
      "epoch = 29, current loss = 6.52, test accuracy = 94.89%\n",
      "epoch = 30, current loss = 12.36, test accuracy = 95.11%\n",
      "epoch = 31, current loss = 5.09, test accuracy = 95.11%\n",
      "epoch = 32, current loss = 9.20, test accuracy = 95.33%\n",
      "epoch = 33, current loss = 5.52, test accuracy = 95.33%\n",
      "epoch = 34, current loss = 3.00, test accuracy = 95.33%\n",
      "epoch = 35, current loss = 7.30, test accuracy = 95.56%\n",
      "epoch = 36, current loss = 8.18, test accuracy = 95.33%\n",
      "epoch = 37, current loss = 8.43, test accuracy = 95.56%\n",
      "epoch = 38, current loss = 5.75, test accuracy = 95.78%\n",
      "epoch = 39, current loss = 5.58, test accuracy = 95.78%\n",
      "epoch = 40, current loss = 7.37, test accuracy = 95.56%\n",
      "Accuracy of my ANN model is 95.56%\n",
      "Takes 1.52s\n"
     ]
    }
   ],
   "source": [
    "import time        \n",
    "# load data\n",
    "X_train, y_train = read_dataset('Digits_X_train.csv', 'Digits_y_train.csv')\n",
    "X_test, y_test = read_dataset('Digits_X_test.csv', 'Digits_y_test.csv')\n",
    "\n",
    "#normlize featurues / encode labels / convert features into one-hot-vector.\n",
    "X_train_norm, X_test_norm = normalize_features(X_train, X_test)\n",
    "y_train_ohe, y_test_ohe = ohe(y_train, y_test)\n",
    "X_train_tensor,X_test_tensor = to_tensor(X_train_norm),to_tensor(X_test_norm)\n",
    "\n",
    "\n",
    "# initialize network\n",
    "myANN1 = Network() \n",
    "\n",
    "#conv;pad; for your homework\n",
    "myANN1.add_layer(FlattenLayer())\n",
    "#myANN.add_layer(LinearLayer(in_num=64, out_num=10,lr=0.0001))\n",
    "myANN1.add_layer(LinearLayer(64,200))\n",
    "myANN1.add_layer(LinearLayer(200,10))\n",
    "myANN1.add_layer(relulayer())\n",
    "myANN1.add_layer(SoftmaxLayer())\n",
    "\n",
    "\n",
    "\n",
    "#start training\n",
    "start = time.time()\n",
    "epoch_num = 40\n",
    "for i in range(epoch_num):\n",
    "    myANN1.mini_batch_descent(X_train_tensor, y_train_ohe, batch_size=200)\n",
    "    myANN1.cross_entropy_loss()\n",
    "    y_pred = myANN1.predict(X_test_tensor)\n",
    "    print('epoch = {}, current loss = {:.2f}, test accuracy = {:.2f}%'.format(i+1, myANN1.loss, 100*accuracy(y_pred, y_test.ravel())))      \n",
    "    \n",
    "end = time.time()\n",
    "y_pred = myANN1.predict(X_test_tensor)\n",
    "print('Accuracy of my ANN model is {:.2f}%'.format(accuracy(y_pred, y_test.ravel())*100))\n",
    "print('Takes {:.2f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "677acab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, current loss = 76.91, test accuracy = 83.56%\n",
      "epoch = 2, current loss = 41.70, test accuracy = 88.00%\n",
      "epoch = 3, current loss = 20.12, test accuracy = 91.56%\n",
      "epoch = 4, current loss = 23.04, test accuracy = 92.44%\n",
      "epoch = 5, current loss = 10.95, test accuracy = 93.56%\n",
      "epoch = 6, current loss = 16.39, test accuracy = 93.33%\n",
      "epoch = 7, current loss = 16.94, test accuracy = 94.22%\n",
      "epoch = 8, current loss = 12.60, test accuracy = 94.00%\n",
      "epoch = 9, current loss = 10.26, test accuracy = 95.11%\n",
      "epoch = 10, current loss = 11.48, test accuracy = 94.67%\n",
      "epoch = 11, current loss = 11.90, test accuracy = 95.56%\n",
      "epoch = 12, current loss = 9.59, test accuracy = 94.89%\n",
      "epoch = 13, current loss = 9.18, test accuracy = 94.67%\n",
      "epoch = 14, current loss = 7.07, test accuracy = 95.11%\n",
      "epoch = 15, current loss = 7.36, test accuracy = 95.11%\n",
      "epoch = 16, current loss = 7.32, test accuracy = 95.33%\n",
      "epoch = 17, current loss = 7.04, test accuracy = 95.33%\n",
      "epoch = 18, current loss = 6.01, test accuracy = 95.56%\n",
      "epoch = 19, current loss = 10.08, test accuracy = 95.56%\n",
      "epoch = 20, current loss = 4.13, test accuracy = 95.78%\n",
      "epoch = 21, current loss = 5.38, test accuracy = 95.78%\n",
      "epoch = 22, current loss = 6.97, test accuracy = 95.56%\n",
      "epoch = 23, current loss = 5.58, test accuracy = 96.00%\n",
      "epoch = 24, current loss = 5.41, test accuracy = 95.78%\n",
      "epoch = 25, current loss = 9.13, test accuracy = 96.00%\n",
      "epoch = 26, current loss = 4.85, test accuracy = 96.00%\n",
      "epoch = 27, current loss = 3.23, test accuracy = 96.00%\n",
      "epoch = 28, current loss = 6.05, test accuracy = 96.00%\n",
      "epoch = 29, current loss = 4.05, test accuracy = 96.00%\n",
      "epoch = 30, current loss = 2.53, test accuracy = 96.00%\n",
      "epoch = 31, current loss = 2.23, test accuracy = 96.22%\n",
      "epoch = 32, current loss = 3.68, test accuracy = 96.00%\n",
      "epoch = 33, current loss = 2.38, test accuracy = 96.22%\n",
      "epoch = 34, current loss = 3.14, test accuracy = 96.00%\n",
      "epoch = 35, current loss = 6.42, test accuracy = 96.00%\n",
      "epoch = 36, current loss = 3.46, test accuracy = 96.00%\n",
      "epoch = 37, current loss = 2.87, test accuracy = 96.00%\n",
      "epoch = 38, current loss = 3.10, test accuracy = 96.00%\n",
      "epoch = 39, current loss = 1.46, test accuracy = 96.00%\n",
      "epoch = 40, current loss = 2.75, test accuracy = 95.78%\n",
      "Accuracy of my ANN model is 95.78%\n",
      "Takes 2.57s\n"
     ]
    }
   ],
   "source": [
    "myANN2 = Network() \n",
    "\n",
    "#conv;pad; for your homework\n",
    "myANN2.add_layer(FlattenLayer())\n",
    "#myANN.add_layer(LinearLayer(in_num=64, out_num=10,lr=0.0001))\n",
    "myANN2.add_layer(LinearLayer(64,100))\n",
    "myANN2.add_layer(LinearLayer(100,100))\n",
    "myANN2.add_layer(LinearLayer(100,10))\n",
    "myANN2.add_layer(relulayer())\n",
    "myANN2.add_layer(SoftmaxLayer())\n",
    "\n",
    "\n",
    "\n",
    "#start training\n",
    "start = time.time()\n",
    "epoch_num = 40\n",
    "for i in range(epoch_num):\n",
    "    myANN2.mini_batch_descent(X_train_tensor, y_train_ohe, batch_size=200)\n",
    "    myANN2.cross_entropy_loss()\n",
    "    y_pred = myANN2.predict(X_test_tensor)\n",
    "    print('epoch = {}, current loss = {:.2f}, test accuracy = {:.2f}%'.format(i+1, myANN2.loss, 100*accuracy(y_pred, y_test.ravel())))      \n",
    "    \n",
    "end = time.time()\n",
    "y_pred = myANN2.predict(X_test_tensor)\n",
    "print('Accuracy of my ANN model is {:.2f}%'.format(accuracy(y_pred, y_test.ravel())*100))\n",
    "print('Takes {:.2f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4bc02e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, current loss = 67.18, test accuracy = 82.44%\n",
      "epoch = 2, current loss = 29.80, test accuracy = 91.33%\n",
      "epoch = 3, current loss = 14.39, test accuracy = 93.11%\n",
      "epoch = 4, current loss = 18.46, test accuracy = 94.00%\n",
      "epoch = 5, current loss = 12.59, test accuracy = 92.44%\n",
      "epoch = 6, current loss = 7.61, test accuracy = 93.33%\n",
      "epoch = 7, current loss = 4.06, test accuracy = 92.67%\n",
      "epoch = 8, current loss = 11.49, test accuracy = 94.67%\n",
      "epoch = 9, current loss = 8.72, test accuracy = 94.44%\n",
      "epoch = 10, current loss = 12.54, test accuracy = 95.11%\n",
      "epoch = 11, current loss = 3.52, test accuracy = 94.22%\n",
      "epoch = 12, current loss = 2.87, test accuracy = 94.44%\n",
      "epoch = 13, current loss = 2.08, test accuracy = 95.11%\n",
      "epoch = 14, current loss = 1.94, test accuracy = 94.67%\n",
      "epoch = 15, current loss = 2.01, test accuracy = 95.11%\n",
      "epoch = 16, current loss = 2.20, test accuracy = 94.44%\n",
      "epoch = 17, current loss = 1.57, test accuracy = 94.89%\n",
      "epoch = 18, current loss = 1.72, test accuracy = 94.67%\n",
      "epoch = 19, current loss = 1.43, test accuracy = 94.44%\n",
      "epoch = 20, current loss = 2.90, test accuracy = 94.67%\n",
      "epoch = 21, current loss = 1.28, test accuracy = 94.89%\n",
      "epoch = 22, current loss = 1.18, test accuracy = 94.44%\n",
      "epoch = 23, current loss = 2.23, test accuracy = 94.44%\n",
      "epoch = 24, current loss = 1.73, test accuracy = 94.67%\n",
      "epoch = 25, current loss = 1.55, test accuracy = 94.67%\n",
      "epoch = 26, current loss = 1.30, test accuracy = 94.89%\n",
      "epoch = 27, current loss = 1.29, test accuracy = 94.44%\n",
      "epoch = 28, current loss = 1.12, test accuracy = 94.22%\n",
      "epoch = 29, current loss = 0.99, test accuracy = 95.11%\n",
      "epoch = 30, current loss = 0.81, test accuracy = 95.11%\n",
      "epoch = 31, current loss = 0.83, test accuracy = 94.22%\n",
      "epoch = 32, current loss = 0.61, test accuracy = 94.67%\n",
      "epoch = 33, current loss = 0.58, test accuracy = 94.89%\n",
      "epoch = 34, current loss = 0.40, test accuracy = 94.44%\n",
      "epoch = 35, current loss = 0.70, test accuracy = 94.67%\n",
      "epoch = 36, current loss = 1.00, test accuracy = 94.67%\n",
      "epoch = 37, current loss = 0.50, test accuracy = 94.67%\n",
      "epoch = 38, current loss = 0.71, test accuracy = 94.44%\n",
      "epoch = 39, current loss = 0.63, test accuracy = 94.89%\n",
      "epoch = 40, current loss = 0.93, test accuracy = 94.67%\n",
      "Accuracy of my ANN model is 94.67%\n",
      "Takes 4.26s\n"
     ]
    }
   ],
   "source": [
    "myANN5 = Network() \n",
    "\n",
    "#conv;pad; for your homework\n",
    "myANN5.add_layer(FlattenLayer())\n",
    "#myANN.add_layer(LinearLayer(in_num=64, out_num=10,lr=0.0001))\n",
    "myANN5.add_layer(LinearLayer(64,50))\n",
    "myANN5.add_layer(LinearLayer(50,100))\n",
    "myANN5.add_layer(LinearLayer(100,100))\n",
    "myANN5.add_layer(LinearLayer(100,100))\n",
    "myANN5.add_layer(LinearLayer(100,50))\n",
    "myANN5.add_layer(LinearLayer(50,10))\n",
    "myANN5.add_layer(relulayer())\n",
    "myANN5.add_layer(SoftmaxLayer())\n",
    "\n",
    "\n",
    "\n",
    "#start training\n",
    "start = time.time()\n",
    "epoch_num = 40\n",
    "for i in range(epoch_num):\n",
    "    myANN5.mini_batch_descent(X_train_tensor, y_train_ohe, batch_size=200)\n",
    "    myANN5.cross_entropy_loss()\n",
    "    y_pred = myANN5.predict(X_test_tensor)\n",
    "    print('epoch = {}, current loss = {:.2f}, test accuracy = {:.2f}%'.format(i+1, myANN5.loss, 100*accuracy(y_pred, y_test.ravel())))      \n",
    "    \n",
    "end = time.time()\n",
    "y_pred = myANN5.predict(X_test_tensor)\n",
    "print('Accuracy of my ANN model is {:.2f}%'.format(accuracy(y_pred, y_test.ravel())*100))\n",
    "print('Takes {:.2f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c4d50",
   "metadata": {},
   "source": [
    "**&#9989; **Problem 2 (60pts)****.  The MNIST datasets are given. Create a CNN with 2 convolution\n",
    "layers and 2 pooling layers to classify the dataset such that:\n",
    "\n",
    "- The first convolution layer with kernel size: (1, filters1, 5, 5), and strides = 1, padding =0.\n",
    "- The second convolution layer with kernel size: (filters1, filters2, 3, 3), and strides = 1, padding = 0.\n",
    "- The pooling size is (2, 2), and the stride size of pooling layers is 2.\n",
    "\n",
    "Machine learning libraries in Python packages should NOT be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d1ed7b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, current loss = 176.87, test accuracy = 48.00%\n",
      "epoch = 2, current loss = 122.00, test accuracy = 56.20%\n",
      "epoch = 3, current loss = 120.20, test accuracy = 63.20%\n",
      "epoch = 4, current loss = 90.45, test accuracy = 67.20%\n",
      "epoch = 5, current loss = 96.14, test accuracy = 70.00%\n",
      "epoch = 6, current loss = 70.36, test accuracy = 71.60%\n",
      "epoch = 7, current loss = 91.77, test accuracy = 73.00%\n",
      "epoch = 8, current loss = 90.87, test accuracy = 74.60%\n",
      "epoch = 9, current loss = 79.53, test accuracy = 74.60%\n",
      "epoch = 10, current loss = 79.85, test accuracy = 75.80%\n",
      "epoch = 11, current loss = 75.59, test accuracy = 76.60%\n",
      "epoch = 12, current loss = 69.48, test accuracy = 76.60%\n",
      "epoch = 13, current loss = 66.65, test accuracy = 78.20%\n",
      "epoch = 14, current loss = 53.86, test accuracy = 78.60%\n",
      "epoch = 15, current loss = 66.24, test accuracy = 79.40%\n",
      "epoch = 16, current loss = 82.79, test accuracy = 79.60%\n",
      "epoch = 17, current loss = 71.43, test accuracy = 79.80%\n",
      "epoch = 18, current loss = 71.61, test accuracy = 80.80%\n",
      "epoch = 19, current loss = 48.22, test accuracy = 82.00%\n",
      "epoch = 20, current loss = 55.00, test accuracy = 81.80%\n",
      "epoch = 21, current loss = 59.50, test accuracy = 82.40%\n",
      "epoch = 22, current loss = 62.50, test accuracy = 82.60%\n",
      "epoch = 23, current loss = 47.76, test accuracy = 82.80%\n",
      "epoch = 24, current loss = 55.81, test accuracy = 83.60%\n",
      "epoch = 25, current loss = 54.53, test accuracy = 83.20%\n",
      "epoch = 26, current loss = 84.17, test accuracy = 82.80%\n",
      "epoch = 27, current loss = 47.05, test accuracy = 83.40%\n",
      "epoch = 28, current loss = 45.33, test accuracy = 83.20%\n",
      "epoch = 29, current loss = 57.16, test accuracy = 83.60%\n",
      "epoch = 30, current loss = 46.18, test accuracy = 83.20%\n",
      "epoch = 31, current loss = 51.98, test accuracy = 84.00%\n",
      "epoch = 32, current loss = 54.61, test accuracy = 83.40%\n",
      "epoch = 33, current loss = 51.43, test accuracy = 83.20%\n",
      "epoch = 34, current loss = 68.21, test accuracy = 83.00%\n",
      "epoch = 35, current loss = 42.55, test accuracy = 83.40%\n",
      "epoch = 36, current loss = 47.05, test accuracy = 83.20%\n",
      "epoch = 37, current loss = 39.80, test accuracy = 82.40%\n",
      "epoch = 38, current loss = 29.59, test accuracy = 83.20%\n",
      "epoch = 39, current loss = 37.90, test accuracy = 83.20%\n",
      "epoch = 40, current loss = 60.31, test accuracy = 83.00%\n",
      "Accuracy of my CNN model is 83.00%\n",
      "Takes 1411.35s\n"
     ]
    }
   ],
   "source": [
    "import time        \n",
    "# load data\n",
    "X_train, y_train = read_dataset('MNIST_X_train2.csv', 'MNIST_y_train2.csv')\n",
    "X_test, y_test = read_dataset('MNIST_X_test2.csv','MNIST_y_test2.csv')\n",
    "\n",
    "#normlize featurues / encode labels / convert features into one-hot-vector.\n",
    "X_train_norm, X_test_norm = normalize_features(X_train, X_test)\n",
    "y_train_ohe, y_test_ohe = ohe(y_train, y_test)\n",
    "X_train_tensor,X_test_tensor = to_tensor(X_train_norm),to_tensor(X_test_norm)\n",
    "\n",
    "#print(X_train_tensor.shape)\n",
    "\n",
    "# initialize network\n",
    "myCNN = Network() \n",
    "myCNN.add_layer(ConvLayer1(lr=0.0001))\n",
    "myCNN.add_layer(MaxPoolingLayer())\n",
    "myCNN.add_layer(ConvLayer2(lr=0.0001))\n",
    "myCNN.add_layer(MaxPoolingLayer())\n",
    "\n",
    "#conv;pad; for your homework\n",
    "myCNN.add_layer(FlattenLayer())\n",
    "myCNN.add_layer(LinearLayer(in_num=50, out_num=100,lr=0.0001))\n",
    "myCNN.add_layer(relulayer())\n",
    "myCNN.add_layer(LinearLayer(in_num=100, out_num = y_train_ohe.shape[1],lr=0.0001))\n",
    "myCNN.add_layer(SoftmaxLayer())\n",
    "\n",
    "#start training\n",
    "start = time.time()\n",
    "epoch_num = 40\n",
    "for i in range(epoch_num):\n",
    "    myCNN.mini_batch_descent(X_train_tensor, y_train_ohe, batch_size=100)\n",
    "    myCNN.cross_entropy_loss()\n",
    "    y_pred = myCNN.predict(X_test_tensor)\n",
    "    print('epoch = {}, current loss = {:.2f}, test accuracy = {:.2f}%'.format(i+1, myCNN.loss, 100*accuracy(y_pred, y_test.ravel())))      \n",
    "    \n",
    "end = time.time()\n",
    "y_pred = myCNN.predict(X_test_tensor)\n",
    "print('Accuracy of my CNN model is {:.2f}%'.format(accuracy(y_pred, y_test.ravel())*100))\n",
    "print('Takes {:.2f}s'.format(end-start))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98653a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6846ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb77cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca3b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b33a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a1ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
