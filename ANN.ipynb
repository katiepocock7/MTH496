{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7613d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "16267cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X_train): ##fit X_train, calculate mean, std of X_train\n",
    "        self.u = X_train.mean(axis=0)\n",
    "        self.s = X_train.std(axis=0)\n",
    "    def transform(self,X):#apply normlization on X_train or X_test\n",
    "        X_norm = (X-self.u)/(self.s+1e-6)## z = (x-u)/s, check: will self.s = 0 ?\n",
    "        return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9007d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class one_hot_encoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,y_train):\n",
    "    #fit y_train, find set{y_train} and assign elements of this set with one-hot-vectors.\n",
    "       label_set = set(y_train)\n",
    "    #assign index to labels:\n",
    "       self.label_list = list(label_set)\n",
    "    \n",
    "    def transform(self,y): #apply one-hot-encoding to y_train or y_test\n",
    "        #the shape of output matrix is (y.shape[0],len(self.label_list))\n",
    "        ohe_matrix = np.zeros((y.shape[0],len(self.label_list)))\n",
    "        for label,vector in zip(y,ohe_matrix):\n",
    "            #find index of label and add 1 to vector[index]\n",
    "            index = self.label_list.index(label)\n",
    "            vector[index]+=1\n",
    "        return ohe_matrix.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6319ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(feature_file,label_file):\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values #convert values in dataframe to numpy 2-D array\n",
    "    y = df_y.values.reshape(-1)   #convert values in dataframe to numpy 1-D array\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "24f0e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X_train,X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    return scaler.transform(X_train),scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f1934d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(y_train,y_test):\n",
    "    ohencoder = one_hot_encoder()\n",
    "    ohencoder.fit(y_train)\n",
    "    return ohencoder.transform(y_train),ohencoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d0caccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_value = np.exp(z-np.amax(z, axis=1, keepdims=True)) # for stablility\n",
    "    # keepdims = True means that the output's dimension is the same as of z\n",
    "    softmax_scores = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "    return softmax_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "376ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ypred, yexact):\n",
    "    p = np.array(ypred == yexact, dtype = int)\n",
    "    return np.sum(p)/float(len(yexact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4e528d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class twolayer_NN:\n",
    "    def __init__(self, X, y, hidden_layer_nn_0=100, hidden_layer_nn_1=100, lr=0.01):\n",
    "        self.X = X # features\n",
    "        self.y = y # labels (targets) in one-hot-encoder\n",
    "        self.lr = lr # learning rate\n",
    "        # Initialize weights\n",
    "        self.nn = X.shape[1] # number of neurons in the input layer\n",
    "\n",
    "        self.Wp = np.random.randn(self.nn, hidden_layer_nn_0) / np.sqrt(self.nn)\n",
    "        self.bp = np.zeros((1, hidden_layer_nn_0)) \n",
    "        \n",
    "        self.W0 = np.random.randn(hidden_layer_nn_0, hidden_layer_nn_1) / np.sqrt(hidden_layer_nn_0)\n",
    "        self.b0 = np.zeros((1, hidden_layer_nn_1)) \n",
    "        \n",
    "        self.output_layer_nn = y.shape[1]\n",
    "        self.Wq = np.random.randn(hidden_layer_nn_1, self.output_layer_nn) / np.sqrt(hidden_layer_nn_1)\n",
    "        self.bq = np.zeros((1, self.output_layer_nn))      \n",
    "        \n",
    "    def feed_forward(self):\n",
    "        # hidden layer\n",
    "        ## z_0 = XW_p + b_p\n",
    "        self.z0 = np.dot(self.X, self.Wp) + self.bp\n",
    "        ## activation function :  f_0 = \\tanh(z_0)\n",
    "        self.f0 = np.tanh(self.z0)\n",
    "        ## z_1 = f_0W_0 + b_0\n",
    "        self.z1 = np.dot(self.f0, self.W0) + self.b0\n",
    "        ## activation function :  f_1 = \\tanh(z_1)\n",
    "        self.f1 = np.tanh(self.z1)\n",
    "        \n",
    "        # output layer\n",
    "        ## z_q = f_1W_q + b_q\n",
    "        self.zq = np.dot(self.f1, self.Wq) + self.bq   \n",
    "        #\\hat{y} = softmax}(z_q)$\n",
    "        self.y_hat = softmax(self.zq)\n",
    "        \n",
    "    def back_propagation(self):\n",
    "        # $d_3 = \\hat{y}-y$\n",
    "        dq = self.y_hat - self.y\n",
    "        # d_2 = (1-f^2_2)*(\\hat{y}-y)W_3^T\n",
    "        d0 = (1-self.f1*self.f1)*(dq.dot((self.Wq).T))\n",
    "        dp = (1-self.f0*self.f0)*(d0.dot((self.W0).T))\n",
    "        \n",
    "        # dL/dWq = f_1^T d3\n",
    "        dWq = np.dot(self.f1.T, dq)\n",
    "        # dL/dbq = sum(dq,axis=0)\n",
    "        dbq = np.sum(dq, axis=0, keepdims=True)\n",
    "        \n",
    "        # dL/dW0 = f_0^T d_0\n",
    "        dW0 = np.dot(self.f0.T, d0)\n",
    "        # dL/b_0 = sum(d_0,axis=0)\n",
    "        db0 = np.sum(d0, axis=0, keepdims=True)\n",
    "        # axis =0 : sum along the vertical axis\n",
    "\n",
    "        # dL/dW_1} = x^T d_1\n",
    "        dWp = np.dot((self.X).T, dp)\n",
    "        # dL/db_1 = d_1.axis=0\n",
    "        dbp = np.sum(dp, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update the gradident descent\n",
    "        self.Wp = self.Wp - self.lr * dWp\n",
    "        self.bp = self.bp - self.lr * dbp\n",
    "        self.W0 = self.W0 - self.lr * dW0\n",
    "        self.b0 = self.b0 - self.lr * db0\n",
    "        self.Wq = self.Wq - self.lr * dWq\n",
    "        self.bq = self.bq - self.lr * dbq\n",
    "        \n",
    "    def cross_entropy_loss(self):\n",
    "        #  $L = -\\sum_n\\sum_{i\\in C} y_{n, i}\\log(\\hat{y}_{n, i})$\n",
    "        # calculate y_hat\n",
    "        self.feed_forward()\n",
    "        self.loss = -np.sum(self.y*np.log(self.y_hat + 1e-6))\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        # Use feed forward to calculat y_hat_test\n",
    "        # hidden layer\n",
    "        ## z_p = XW_p + b_p\n",
    "        z0 = np.dot(X_test, self.Wp) + self.bp\n",
    "        ## activation function :  f_0 = \\tanh(z_0)\n",
    "        f0 = np.tanh(z0)\n",
    "        ## z_1 = f_0W_0 + b_0\n",
    "        z1 = np.dot(f0, self.W0) + self.b0\n",
    "        f1 = np.tanh(z1)\n",
    "        # output layer\n",
    "        ## z_q = f_1W_q + b_q\n",
    "        zq = np.dot(f1, self.Wq) + self.bq    \n",
    "        #\\hat{y} = softmax(z_q)$\n",
    "        y_hat_test = softmax(zq)\n",
    "        # the rest is similar to the logistic regression\n",
    "        labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        num_test_samples = X_test.shape[0]\n",
    "        # find which index gives us the highest probability\n",
    "        ypred = np.zeros(num_test_samples, dtype=int) \n",
    "        for i in range(num_test_samples):\n",
    "            ypred[i] = labels[np.argmax(y_hat_test[i,:])]\n",
    "        return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e0ff1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_dataset('MNIST_X_train.csv', 'MNIST_y_train.csv')\n",
    "X_test, y_test = read_dataset('MNIST_X_test.csv', 'MNIST_y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ddac160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20, current loss = 162.89495\n",
      "epoch = 40, current loss = 48.29837\n",
      "epoch = 60, current loss = 22.92000\n",
      "epoch = 80, current loss = 14.14362\n",
      "epoch = 100, current loss = 9.99184\n",
      "epoch = 120, current loss = 7.62937\n",
      "epoch = 140, current loss = 6.12242\n",
      "epoch = 160, current loss = 5.08525\n",
      "epoch = 180, current loss = 4.33173\n",
      "epoch = 200, current loss = 3.76168\n",
      "Accuracy of our model  0.896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_norm, X_test_norm = normalize_features(X_train, X_test)\n",
    "y_train_ohe, y_test_ohe = ohe(y_train, y_test)\n",
    "# \n",
    "myNN = twolayer_NN(X_train_norm, y_train_ohe, hidden_layer_nn_0=300, hidden_layer_nn_1=100, lr=0.001)  \n",
    "epoch_num = 200\n",
    "for i in range(epoch_num):\n",
    "    myNN.feed_forward()\n",
    "    myNN.back_propagation()\n",
    "    myNN.cross_entropy_loss()\n",
    "    if ((i+1)%20 == 0):\n",
    "        print('epoch = %d, current loss = %.5f' % (i+1, myNN.loss))         \n",
    "        \n",
    "y_pred = myNN.predict(X_test_norm)\n",
    "print('Accuracy of our model ', accuracy(y_pred, y_test.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cd081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0a5e3b595277012da1d4e6f58daa3949247c6b2eb723fb51e592c2a520cd22ff6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
